{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diabetes in the Pima People"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "#import data\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "#change float64 values to integers for comparisons to work correctly\n",
    "data.loc[:,['BMI','DiabetesPedigreeFunction']]=data.loc[:,['BMI','DiabetesPedigreeFunction']].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the dataset, there were no missing datapoints as None or NaN. However, many of the features are indeed physical attributes and there are many 0 values. With the columns for glucose (concentration in blood), blood pressure, skin thickness, insulin (concentration in blood), BMI (a height and weight measurement), and age I decided if 3 or more of these are 0 in any row then they should be left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for columns glucose through age, remove rows with 3 or more 0's across those columns\n",
    "data = data.loc[(data.loc[:,'Glucose':'Age']==0).sum(axis=1)<3,:]\n",
    "\n",
    "#split independent and dependent\n",
    "x = data.iloc[:,0:8].values\n",
    "y = data.iloc[:,8].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent a warning when scaling, the x variable will by transformed to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change all x values to float for imputing and scaling\n",
    "x = x.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the rows with 3 or more 0 values in the aforementioned column are gone, I assumed it to be safe to replace the remaining few 0 values with the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace remaining glucose and skin thickness 0's with the respective mean's\n",
    "from sklearn.preprocessing import Imputer\n",
    "xputer = Imputer(missing_values = 0, strategy = 'mean', axis = 0)\n",
    "xputer = xputer.fit(x[:,[1,3]])\n",
    "x[:,[1,3]] = xputer.transform(x[:,[1,3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be split into training and test sets, then features scaled to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train test split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "#feature scaling x\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "xscaler = StandardScaler()\n",
    "xtrain = xscaler.fit_transform(xtrain)\n",
    "xtest = xscaler.transform(xtest)\n",
    "\n",
    "#fit model\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classifier trained, we are ready to make predictions on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict\n",
    "ypred = classifier.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the confusion matrix we can see how will it performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[69  7]\n",
      " [14 19]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(ytest,ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this as a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single score: 80.73%\n"
     ]
    }
   ],
   "source": [
    "#single score\n",
    "score = classifier.score(xtest,ytest)\n",
    "print('single score: {0:.2f}%'.format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this instance the classifier does a very decent job of predicting the outcomes correctly. Now we will see a more realistic score with 5 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cross-validation score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(classifier, x, y, cv=5)\n",
    "print('cv score: {0:.2f}% +/- {1:.2f}%'.format(scores.mean()*100,scores.std()*200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is lower than the single score but give's more insight to how well the classifer performs which is still very decent. The model may be optimized more with better hyperparameters but the purpose here was to demontrate the process of building the model. Also, I did run logistic regression, k-nearest neigbors, and random forest classifiers but they did not perform as well as the linear support vector machine classifier.\n",
    "\n",
    "Dataset - https://www.kaggle.com/uciml/pima-indians-diabetes-database"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
